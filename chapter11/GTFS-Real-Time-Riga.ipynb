{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cfdfb4",
   "metadata": {},
   "source": [
    "## 11.4. GTFS Realtime Analysis of the City of Riga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb15fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gtfs-realtime-bindings requests pandas plotly pyarrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyarrow==10.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5759070-8096-499b-8bd7-0be49881ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyarrow \n",
    "#Not working for Mac M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e830cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from google.transit import gtfs_realtime_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2615fa",
   "metadata": {},
   "source": [
    "# 11.4. GTFS-Realtime URL for Riga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51d3bf",
   "metadata": {},
   "source": [
    "# Section 11.4.1. Part 1 - Ingesting GTFS Realtime Feed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9be94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.4. GTFS-Realtime URL for Riga\n",
    "# Section 11.4.1. - Ingesting GTFS Realtime Feed - Part 1\n",
    "\n",
    "GTFS_REALTIME_URL = \"https://saraksti.rigassatiksme.lv/gtfs_realtime.pb\"\n",
    "\n",
    "# Function to fetch GTFS-Realtime data\n",
    "def fetch_gtfs_realtime_data(url):\n",
    "    \"\"\"\n",
    "    Fetches the GTFS-Realtime data from the given URL with SSL verification disabled.\n",
    "    \n",
    "    Parameters:\n",
    "    - url: The GTFS-Realtime endpoint URL\n",
    "    \n",
    "    Returns:\n",
    "    - feed: Parsed GTFS-Realtime protobuf feed\n",
    "    \"\"\"\n",
    "    feed = gtfs_realtime_pb2.FeedMessage()\n",
    "    \n",
    "    try:\n",
    "        # Disable SSL verification\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "        # Parse the protobuf data\n",
    "        feed.ParseFromString(response.content)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GTFS-Realtime data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc42368",
   "metadata": {},
   "source": [
    "# Section 11.4.1. Part 2 - Function to extract vehicle positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183546d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.4.1. - Part 2 - Function to extract vehicle positions \n",
    "\n",
    "def extract_vehicle_positions(feed):\n",
    "    \"\"\"\n",
    "    Extracts vehicle positions from the GTFS-Realtime feed.\n",
    "    \n",
    "    Parameters:\n",
    "    - feed: Parsed GTFS-Realtime protobuf feed\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing vehicle positions and trip information\n",
    "    \"\"\"\n",
    "    vehicle_position = []\n",
    "    for entity in feed.entity:\n",
    "        if entity.HasField('vehicle'):\n",
    "            vehicle = entity.vehicle\n",
    "            vehicle_position.append({\n",
    "                \"id\": entity.id,\n",
    "                \"trip_id\": vehicle.trip.trip_id,\n",
    "                \"schedule_relationship\": vehicle.trip.schedule_relationship,\n",
    "                \"latitude\": vehicle.position.latitude,\n",
    "                \"longitude\": vehicle.position.longitude,\n",
    "                \"bearing\": vehicle.position.bearing,\n",
    "                \"speed\": vehicle.position.speed * 3.6,  # Speed in km/h\n",
    "                \"current_status\": vehicle.current_status,\n",
    "                \"timestamp\": vehicle.timestamp,\n",
    "                \"stop_id\": vehicle.stop_id,\n",
    "                \"vehicle_id\": vehicle.vehicle.id,\n",
    "                \"vehicle_label\": vehicle.vehicle.label,\n",
    "                \"vehicle_license_plate\": vehicle.vehicle.license_plate       \n",
    "            })\n",
    "    return vehicle_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550b68f",
   "metadata": {},
   "source": [
    "# 11.4.2. Parquet Files and DuckDB for Mobility Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.4.2. Parquet Files and DuckDB for Mobility Data\n",
    "# Function to collect vehicle positions for a specific duration and save into a Parquet File\n",
    "\n",
    "def collect_vehicle_positions(duration_minutes, interval_seconds):\n",
    "    \"\"\"\n",
    "    Collects vehicle positions from the GTFS-Realtime endpoint at regular intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    - duration_minutes: Duration of the collection period in minutes\n",
    "    - interval_seconds: Interval between successive API calls in seconds\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing all collected vehicle positions\n",
    "    \"\"\"\n",
    "    collected_data = []\n",
    "    end_time = time.time() + duration_minutes * 60  # Convert minutes to seconds\n",
    "\n",
    "    while time.time() < end_time:\n",
    "        # Fetch the GTFS-Realtime data\n",
    "        feed = fetch_gtfs_realtime_data(GTFS_REALTIME_URL)\n",
    "\n",
    "        # If feed is fetched, extract vehicle positions\n",
    "        if feed:\n",
    "            vehicle_positions = extract_vehicle_positions(feed)\n",
    "            if vehicle_positions:\n",
    "                collected_data.extend(vehicle_positions)  # Add the new data to the list\n",
    "                print(f\"Collected {len(vehicle_positions)} vehicle positions.\")\n",
    "            else:\n",
    "                print(\"No vehicle positions found in the feed.\")\n",
    "        else:\n",
    "            print(\"Failed to fetch the GTFS-Realtime feed.\")\n",
    "        \n",
    "        # Wait for the specified interval before making the next request\n",
    "        time.sleep(interval_seconds)\n",
    "    \n",
    "    # Convert the collected data into a DataFrame\n",
    "    df = pd.DataFrame(collected_data)\n",
    "    return df\n",
    "\n",
    "# Function to save DataFrame to a Parquet file\n",
    "def save_to_parquet(df, file_name):\n",
    "    \"\"\"\n",
    "    Saves the collected vehicle data to a Parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the vehicle positions data\n",
    "    - file_name: The name of the output Parquet file\n",
    "    \"\"\"\n",
    "    if not df.empty:\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, file_name)\n",
    "        print(f\"Data successfully saved to {file_name}\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "        \n",
    "        \n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    DURATION_MINUTES = 2  # Keep collecting data for this duration\n",
    "    INTERVAL_SECONDS = 10   # Call the endpoint every 10 seconds\n",
    "\n",
    "    # Collect vehicle positions over the specified duration\n",
    "    print(f\"Starting data collection for {DURATION_MINUTES} minutes, querying every {INTERVAL_SECONDS} seconds...\")\n",
    "    vehicle_positions_df = collect_vehicle_positions(DURATION_MINUTES, INTERVAL_SECONDS)\n",
    "\n",
    "    # Save the data to a Parquet file\n",
    "    output_file = \"riga_vehicle_positions_test.parquet\"\n",
    "    save_to_parquet(vehicle_positions_df, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b91fa",
   "metadata": {},
   "source": [
    "# 11.4.3. Querying and Visualizing Vehicle Trajectories with Parquet Files and DuckDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2132b-3205-48ba-a64c-5dace2f1869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# Path to the Parquet file\n",
    "parquet_file = \"riga_vehicle_positions.parquet\"\n",
    "\n",
    "# Function to load the parquet file and query the trajectory of Tram 1\n",
    "def query_tram1_trajectory(parquet_file):\n",
    "    \"\"\"\n",
    "    Queries the trajectory of Tram 1 from the parquet file using DuckDB.\n",
    "    \n",
    "    Parameters:\n",
    "    - parquet_file: Path to the Parquet file\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the trajectory of Tram 1\n",
    "    \"\"\"\n",
    "    # Connect to DuckDB in-memory database\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    # Load the parquet file and query for Tram 1 (Assuming Tram 1 is identified by route_id or trip_id)\n",
    "    # Adjust the condition to match the identifier for Tram 1\n",
    "    query = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM parquet_scan('{parquet_file}')\n",
    "    WHERE trip_id LIKE '%TRAM1-%'  -- Adjust based on actual trip_id or route_id for Tram 1\n",
    "    ORDER BY timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query and return the result as a pandas DataFrame\n",
    "    df = con.execute(query).fetchdf()\n",
    "\n",
    "    # Close the connection\n",
    "    con.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "import folium\n",
    "\n",
    "def visualize_trajectory(df):\n",
    "    \"\"\"\n",
    "    Visualizes the trajectory of Tram 1 using Folium with OpenStreetMap tiles.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the trajectory of Tram 1\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for Tram 1.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize a Folium map centered on Riga\n",
    "    m = folium.Map(location=[56.9496, 24.1052], \n",
    "                   tiles=\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",\n",
    "                   attr=\"OpenStreetMap\", zoom_start=12)\n",
    "\n",
    "    # Add trajectory points to the map\n",
    "    for _, row in df.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=4,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            fill_opacity=0.7,\n",
    "            tooltip=f\"Vehicle: {row['vehicle_id']}<br>Trip: {row['trip_id']}<br>Speed: {row['speed']}<br>Timestamp: {row['timestamp']}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and query the trajectory of Tram 1\n",
    "    tram1_df = query_tram1_trajectory(parquet_file)\n",
    "    \n",
    "    if not tram1_df.empty:\n",
    "        # Visualize the trajectory of Tram 1\n",
    "        m= visualize_trajectory(tram1_df)\n",
    "        display(m)\n",
    "    else:\n",
    "        print(\"No data found for Tram 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea4358",
   "metadata": {},
   "source": [
    "# 11.4.4 Querying and Visualizing Vehicle Trajectories with PostgreSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87777c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11.4.4 Querying and Visualizing Vehicle Trajectories with PostgreSQL\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg\n",
    "import json\n",
    "\n",
    "# Function to load database configuration from config.json\n",
    "def load_db_config(config_path=\"config.json\"):\n",
    "    \"\"\"\n",
    "    Loads database configuration from a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - config_path: Path to the configuration JSON file.\n",
    "    \n",
    "    Returns:\n",
    "    - config: Dictionary containing database connection details\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Function to connect to the PostgreSQL database\n",
    "def connect_to_postgres(config):\n",
    "    \"\"\"\n",
    "    Establishes a connection to the PostgreSQL database using the configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    - config: Dictionary containing database connection details\n",
    "    \n",
    "    Returns:\n",
    "    - conn: The PostgreSQL connection object\n",
    "    - cur: The cursor object for executing SQL queries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg.connect(\n",
    "            host=config[\"DB_HOST\"],\n",
    "            port=config[\"DB_PORT\"],\n",
    "            dbname=config[\"DB_NAME\"],\n",
    "            user=config[\"DB_USER\"],\n",
    "            password=config[\"DB_PASS\"]\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        return conn, cur\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to create the 'vehiclePosition' table in PostgreSQL\n",
    "\n",
    "def create_vehicle_position_table(cur):\n",
    "    \"\"\"\n",
    "    Creates the 'vehiclePosition' table in PostgreSQL if it doesn't exist.\n",
    "    \n",
    "    Parameters:\n",
    "    - cur: The cursor object for executing SQL queries\n",
    "    \"\"\"\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS vehiclePosition (\n",
    "        id TEXT,\n",
    "        trip_id TEXT,\n",
    "        schedule_relationship TEXT,\n",
    "        latitude FLOAT8,\n",
    "        longitude FLOAT8,\n",
    "        bearing FLOAT8,\n",
    "        speed FLOAT8,  -- Speed in kilometers per hour (km/h)\n",
    "        current_status TEXT,\n",
    "        timestamp BIGINT,\n",
    "        stop_id TEXT,\n",
    "        vehicle_id TEXT,\n",
    "        vehicle_label TEXT,\n",
    "        vehicle_license_plate TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "    cur.execute(create_table_query)\n",
    "\n",
    "# Function to load the Parquet file into a Pandas DataFrame\n",
    "def load_parquet_file(parquet_file):\n",
    "    \"\"\"\n",
    "    Loads the Parquet file into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - parquet_file: Path to the Parquet file\n",
    "    \n",
    "    Returns:\n",
    "    - df: DataFrame containing the vehicle positions data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Parquet file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to insert the DataFrame data into the PostgreSQL 'vehiclePosition' table\n",
    "def insert_data_to_postgres(df, cur, conn):\n",
    "    \"\"\"\n",
    "    Inserts the data from the DataFrame into the PostgreSQL 'vehiclePosition' table.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the vehicle positions data\n",
    "    - cur: The cursor object for executing SQL queries\n",
    "    - conn: The PostgreSQL connection object\n",
    "    \"\"\"\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO vehiclePosition (id, trip_id, schedule_relationship, latitude, longitude, bearing, speed,\n",
    "                                 current_status, timestamp, stop_id, vehicle_id, vehicle_label, vehicle_license_plate)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over the DataFrame rows and insert them into the database\n",
    "    for _, row in df.iterrows():\n",
    "        cur.execute(insert_query, (\n",
    "            row['id'],\n",
    "            row['trip_id'],\n",
    "            row['schedule_relationship'],\n",
    "            row['latitude'],\n",
    "            row['longitude'],\n",
    "            row['bearing'],\n",
    "            row['speed'],\n",
    "            row['current_status'],\n",
    "            row['timestamp'],\n",
    "            row['stop_id'],\n",
    "            row['vehicle_id'],\n",
    "            row['vehicle_label'],\n",
    "            row['vehicle_license_plate']\n",
    "        ))\n",
    "    \n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the database configuration\n",
    "    db_config = load_db_config(\"config.json\")\n",
    "    \n",
    "    # Step 1: Connect to PostgreSQL\n",
    "    conn, cur = connect_to_postgres(db_config)\n",
    "    if conn is None or cur is None:\n",
    "        print(\"Failed to connect to the PostgreSQL database.\")\n",
    "        exit()\n",
    "    \n",
    "    # Step 2: Create the 'vehiclePosition' table if it doesn't exist\n",
    "    print(\"Creating 'vehiclePosition' table if it doesn't exist...\")\n",
    "    create_vehicle_position_table(cur)\n",
    "    \n",
    "    # Step 3: Load the Parquet file into a DataFrame\n",
    "    print(\"Loading Parquet file...\")\n",
    "    vehicle_positions_df = load_parquet_file(parquet_file)\n",
    "    \n",
    "    if not vehicle_positions_df.empty:\n",
    "        print(f\"Loaded {len(vehicle_positions_df)} rows from the Parquet file.\")\n",
    "        \n",
    "        # Step 4: Insert the data into the PostgreSQL 'vehiclePosition' table\n",
    "        print(\"Inserting data into PostgreSQL...\")\n",
    "        insert_data_to_postgres(vehicle_positions_df, cur, conn)\n",
    "        print(\"Data insertion complete.\")\n",
    "    else:\n",
    "        print(\"No data found in the Parquet file.\")\n",
    "    \n",
    "    # Step 5: Close the PostgreSQL connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"PostgreSQL connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc927b9",
   "metadata": {},
   "source": [
    "# 11.4.4 Dash Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.4.4. Dash Visualizations\n",
    "\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "# Function to fetch available trip_ids from PostgreSQL\n",
    "def fetch_trip_ids():\n",
    "    # Load the database configuration\n",
    "    db_config = load_db_config(\"config.json\")\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    conn, cur = connect_to_postgres(db_config)\n",
    "\n",
    "    if conn is None or cur is None:\n",
    "        return []\n",
    "\n",
    "    trip_id_query = \"SELECT DISTINCT trip_id FROM vehiclePosition;\"\n",
    "    cur.execute(trip_id_query)\n",
    "    trip_ids = [row[0] for row in cur.fetchall()]\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return trip_ids\n",
    "\n",
    "# Function to fetch vehicle positions for a specific trip_id\n",
    "def fetch_vehicle_positions(trip_id):\n",
    "    # Load the database configuration\n",
    "    db_config = load_db_config(\"config.json\")\n",
    "    \n",
    "    # Connect to PostgreSQL\n",
    "    conn, cur = connect_to_postgres(db_config)\n",
    "\n",
    "    if conn is None or cur is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT latitude, longitude, vehicle_id, speed, timestamp\n",
    "    FROM vehiclePosition\n",
    "    WHERE trip_id = %s\n",
    "    ORDER BY timestamp;\n",
    "    \"\"\"\n",
    "    \n",
    "    cur.execute(query, (trip_id,))\n",
    "    vehicle_positions = pd.DataFrame(cur.fetchall(), columns=['latitude', 'longitude', 'vehicle_id', 'speed', 'timestamp'])\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return vehicle_positions\n",
    "\n",
    "# Dash app layout\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "# Get initial trip_ids to populate the dropdown\n",
    "trip_ids = fetch_trip_ids()\n",
    "\n",
    "# Set default trip_id (first one in the list)\n",
    "initial_trip_id = trip_ids[0] if trip_ids else None\n",
    "\n",
    "app.layout = dbc.Container([\n",
    "    html.H1(\"Vehicle Position Visualization\"),\n",
    "    \n",
    "    # Dropdown for selecting trip_id\n",
    "    dcc.Dropdown(\n",
    "        id=\"trip-dropdown\",\n",
    "        options=[{'label': trip_id, 'value': trip_id} for trip_id in trip_ids],\n",
    "        value=initial_trip_id,  # Default to the first trip_id\n",
    "        clearable=False,\n",
    "        style={\"width\": \"80%\"}\n",
    "    ),\n",
    "    \n",
    "    # Graph to display vehicle positions\n",
    "    dcc.Graph(id=\"vehicle-map\")\n",
    "])\n",
    "\n",
    "# Callback to update map based on selected trip_id\n",
    "@app.callback(\n",
    "    Output(\"vehicle-map\", \"figure\"),\n",
    "    Input(\"trip-dropdown\", \"value\")\n",
    ")\n",
    "def update_map(trip_id):\n",
    "    if trip_id is None:\n",
    "        return px.scatter_mapbox()  # Empty map if no trip_id is selected\n",
    "\n",
    "    # Fetch vehicle positions for the selected trip_id\n",
    "    vehicle_positions_df = fetch_vehicle_positions(trip_id)\n",
    "    \n",
    "    if vehicle_positions_df.empty:\n",
    "        return px.scatter_mapbox()  # Return empty map if no data\n",
    "\n",
    "    # Create a map visualization using Plotly\n",
    "    fig = px.scatter_mapbox(vehicle_positions_df, \n",
    "                            lat=\"latitude\", lon=\"longitude\", \n",
    "                            hover_name=\"vehicle_id\", \n",
    "                            hover_data=[\"speed\", \"timestamp\"],\n",
    "                            title=f\"Vehicle Positions for {trip_id}\",\n",
    "                            zoom=12)\n",
    "    fig.update_traces(marker=dict(size=10, color='red'))  # Increase the size and set color to red\n",
    "        \n",
    "    fig.update_layout(mapbox_style=\"open-street-map\", \n",
    "                      mapbox_center={\"lat\": vehicle_positions_df[\"latitude\"].mean(), \n",
    "                                     \"lon\": vehicle_positions_df[\"longitude\"].mean()},\n",
    "                      margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Main function to run the Dash app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True, port=8050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0341b9",
   "metadata": {},
   "source": [
    "# 11.5.1. Speed Analysis Over Network Segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfb4cf-5a7d-425a-95b1-7d73d8e13660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import geopandas as gpd\n",
    "import folium as fl\n",
    "from folium.features import GeoJson, GeoJsonTooltip\n",
    "import branca.colormap as cm\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load the database configuration from a JSON file\n",
    "def load_db_config(config_path=\"config.json\"):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Create a SQLAlchemy engine using the database configuration\n",
    "def create_sqlalchemy_engine(config):\n",
    "    db_url = (\n",
    "        f\"postgresql://{config['DB_USER']}:{config['DB_PASS']}@\"\n",
    "        f\"{config['DB_HOST']}:{config['DB_PORT']}/{config['DB_NAME']}\"\n",
    "    )\n",
    "    return create_engine(db_url)\n",
    "\n",
    "# Fetch data from PostgreSQL and return a GeoDataFrame\n",
    "def fetch_segment_speed_data(engine):\n",
    "    \"\"\"\n",
    "    Fetches the average speed per segment from PostgreSQL and returns a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine for database connection.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame containing segment speed and geometries.\n",
    "    \"\"\"\n",
    "    sql_query = \"\"\"\n",
    "    SELECT\n",
    "        AVG(s.distance_m / EXTRACT(EPOCH FROM (ts.end_time_actual - ts.start_time_actual)) * 3.6) AS speedKMH,\n",
    "        s.geometry,\n",
    "        ts.start_stop_id,\n",
    "        ts.end_stop_id\n",
    "    FROM tripSegments ts\n",
    "    JOIN segments s\n",
    "    ON ts.start_stop_id = s.start_stop_id AND ts.end_stop_id = s.end_stop_id\n",
    "    WHERE ts.start_time_actual IS NOT NULL\n",
    "      AND EXTRACT(EPOCH FROM (ts.end_time_actual - ts.start_time_actual)) > 0\n",
    "    GROUP BY s.geometry, ts.start_stop_id, ts.end_stop_id;\n",
    "    \"\"\"\n",
    "    return gpd.read_postgis(sql_query, engine, geom_col='geometry')\n",
    "\n",
    "# Create a colormap based on speed\n",
    "def create_colormap(min_speed, max_speed):\n",
    "    \"\"\"\n",
    "    Creates a colormap for speed values.\n",
    "\n",
    "    Parameters:\n",
    "    - min_speed: Minimum speed value.\n",
    "    - max_speed: Maximum speed value.\n",
    "\n",
    "    Returns:\n",
    "    - A colormap function.\n",
    "    \"\"\"\n",
    "    return cm.LinearColormap(['red', 'white'], vmin=min_speed, vmax=max_speed)\n",
    "\n",
    "# Visualize the speed per segment using folium\n",
    "def visualize_speed_map(gdf, cutoff=35):\n",
    "    \"\"\"\n",
    "    Visualizes the average speed per segment using folium.\n",
    "\n",
    "    Parameters:\n",
    "    - gdf: GeoDataFrame containing segment geometries and speedKMH.\n",
    "    - cutoff: Maximum speed value for the color gradient.\n",
    "\n",
    "    Returns:\n",
    "    - A folium map with segments color-coded by speedKMH.\n",
    "    \"\"\"\n",
    "    transport_map = fl.Map(\n",
    "        location=[56.9496, 24.1052],  # Centered around Riga\n",
    "        zoom_start=12,\n",
    "        tiles=\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",\n",
    "        attr=\"OpenStreetMap\"\n",
    "    )\n",
    "\n",
    "    # Create a colormap for the speeds\n",
    "    min_speed = gdf['speedkmh'].min()\n",
    "    max_speed = min(gdf['speedkmh'].max(), cutoff)\n",
    "    colormap = create_colormap(min_speed, max_speed)\n",
    "\n",
    "    # Add each segment to the map\n",
    "    for _, segment in gdf.iterrows():\n",
    "        speed_kmh = min(segment['speedkmh'], cutoff)\n",
    "        color = colormap(speed_kmh)\n",
    "\n",
    "        geo_json = GeoJson(\n",
    "            data={\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": segment['geometry'].__geo_interface__,\n",
    "                \"properties\": {\n",
    "                    \"speedkmh\": round(segment['speedkmh'], 2),\n",
    "                    \"from_stop_id\": segment['start_stop_id'],\n",
    "                    \"to_stop_id\": segment['end_stop_id']\n",
    "                }\n",
    "            },\n",
    "            style_function=lambda x, color=color: {\n",
    "                'color': color,\n",
    "                'weight': 3,\n",
    "                'opacity': 0.7\n",
    "            },\n",
    "            tooltip=GeoJsonTooltip(\n",
    "                fields=['speedkmh', 'from_stop_id', 'to_stop_id'],\n",
    "                aliases=['Speed (km/h)', 'From Stop', 'To Stop'],\n",
    "                localize=True\n",
    "            )\n",
    "        )\n",
    "        geo_json.add_to(transport_map)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    colormap.add_to(transport_map)\n",
    "    return transport_map\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load database configuration\n",
    "    db_config = load_db_config(\"config.json\")\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_sqlalchemy_engine(db_config)\n",
    "\n",
    "    # Fetch segment speed data\n",
    "    segment_speed_gdf = fetch_segment_speed_data(engine)\n",
    "\n",
    "    # Visualize the average speed per segment\n",
    "    speed_map = visualize_speed_map(segment_speed_gdf)\n",
    "\n",
    "    # Display the map\n",
    "    speed_map.save(\"segment_speed_map.html\")\n",
    "    print(\"Map saved as 'segment_speed_map.html'.\")\n",
    "\n",
    "    # display the map\n",
    "    display(speed_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8d045",
   "metadata": {},
   "source": [
    "# 11.5.2 Delay Analysis in Public Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.2 Delay Analysis in Public Transport\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from psycopg import connect\n",
    "\n",
    "# Function to load database configuration from config.json\n",
    "def load_db_config(config_path=\"config.json\"):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to connect to the PostgreSQL database\n",
    "def connect_to_postgres(config):\n",
    "    try:\n",
    "        conn = connect(\n",
    "            host=config[\"DB_HOST\"],\n",
    "            port=config[\"DB_PORT\"],\n",
    "            dbname=config[\"DB_NAME\"],\n",
    "            user=config[\"DB_USER\"],\n",
    "            password=config[\"DB_PASS\"]\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        return conn, cur\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Function to fetch the delay data from the database\n",
    "def fetch_delay_data(config):\n",
    "    conn, cur = connect_to_postgres(config)\n",
    "    if conn is None or cur is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    query = \"\"\"\n",
    "    WITH noduptrip AS(\n",
    "\t  SELECT distinct(t.*) \n",
    "\t  FROM tripStops t),\n",
    "   tripDelay AS(\n",
    "        SELECT actual_trip_id, schedule_trip_id,\n",
    "            tgeompointSeq(array_agg(tgeompoint(stop_loc, schedule_time) ORDER BY schedule_time)) AS schedule_trip,\n",
    "            tgeompointSeq(array_agg(tgeompoint(trip_Geom, actual_time) ORDER BY actual_time)) AS actual_trip\n",
    "        FROM noduptrip\n",
    "        WHERE actual_trip_id like 'TROL4-%'\n",
    "        GROUP BY actual_trip_id, schedule_trip_id),\n",
    "    Final AS (SELECT actual_trip_id, schedule_trip_id, extract(EPOCH FROM actual - schedule)/60 as delay, actual AS t\n",
    "    FROM  (\n",
    "      SELECT actual_trip_id, schedule_trip_id,\n",
    "        unnest(timestamps(actual_trip)) AS actual,\n",
    "        unnest(timestamps(schedule_trip)) AS schedule\n",
    "      FROM tripDelay\n",
    "    ) x)\n",
    "    SELECT * \n",
    "    FROM Final\n",
    "    WHERE delay < 12;\n",
    "    \"\"\"\n",
    "    \n",
    "    cur.execute(query)\n",
    "    delay_data = pd.DataFrame(cur.fetchall(), columns=['actual_trip_id', 'schedule_trip_id', 'delay', 't'])\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return delay_data\n",
    "\n",
    "# Function to plot all trips on the same plot\n",
    "def plot_all_trips(delay_data):\n",
    "    \"\"\"\n",
    "    Plots all delay functions for each trip in one plot.\n",
    "    \n",
    "    Parameters:\n",
    "    - delay_data: DataFrame containing delay data for all trips\n",
    "    \"\"\"\n",
    "    # Create a line plot where color represents each trip\n",
    "    fig = px.line(delay_data, x='t', y='delay', color='actual_trip_id',\n",
    "                  title=\"\",\n",
    "                  labels={\"t\": \"Time\", \"delay\": \"Delay (minutes)\", \"actual_trip_id\": \"Trip ID\"},\n",
    "                  markers=True)\n",
    "    \n",
    "    # Customize the layout\n",
    "    fig.update_layout(xaxis_title=\"Time\", yaxis_title=\"Delay (minutes)\",\n",
    "                      xaxis=dict(tickformat=\"%H:%M:%S\"),\n",
    "                      template=\"plotly_white\")\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load database configuration\n",
    "    db_config = load_db_config(\"config.json\")\n",
    "\n",
    "    # Fetch delay data\n",
    "    delay_data = fetch_delay_data(db_config)\n",
    "    \n",
    "    if delay_data.empty:\n",
    "        print(\"No delay data found.\")\n",
    "    else:\n",
    "        # Step 2: Plot all delay functions in one plot\n",
    "        plot_all_trips(delay_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
